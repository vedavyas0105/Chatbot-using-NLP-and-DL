{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64915981-69ad-4e95-9af3-195298f38cef",
   "metadata": {},
   "source": [
    "# Chatbot using NLP and Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8084d8b-8b3d-4b37-a725-e28b8ed0cc69",
   "metadata": {},
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "85aac9b0-7b21-49ef-8952-bd59647a7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd33ea07-f5a8-48c2-af61-68279e6ce6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# Only download for the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b08ece-e0c4-4d1e-8d43-c38cda5d265b",
   "metadata": {},
   "source": [
    "We should download the tokenizer if we are working for the first time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec94d46-22ba-4541-8378-943b05e46c8f",
   "metadata": {},
   "source": [
    "Our NLP Preprocessing pipeline for creating the training data\n",
    "\n",
    "1. Tokenization\n",
    "2. Lowering and Stemming\n",
    "3. Removing Punctuations\n",
    "4. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d762fc-b174-47b2-8f35-c838399a1631",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1e68d-a084-423f-9328-607eeb5d8bb3",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking text into smaller units, called **tokens** (words, subwords, or sentences), which are used as input for NLP models.  \n",
    "\n",
    "For example, in the sentence:  \n",
    "*\"Machine Learning is amazing!\"*  \n",
    "\n",
    "**Word tokenization** → `[\"Machine\", \"Learning\", \"is\", \"amazing\", \"!\"]`  \n",
    "\n",
    "It helps models understand and process text efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53af2cd7-1a59-4113-a80d-37b1deee4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265068c-58e9-4e45-9d23-6f1df0685c48",
   "metadata": {},
   "source": [
    "We are using the `word_tokenize` method from NLTK to split the given sentence into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d17a9ab-ebe3-456b-9f44-d3fb82696ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! How are you doing?\n"
     ]
    }
   ],
   "source": [
    "a = \"Hi! How are you doing?\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ed289d8-bf7d-45d4-8d0b-24390166354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', '!', 'How', 'are', 'you', 'doing', '?']\n"
     ]
    }
   ],
   "source": [
    "a=tokenize(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633141a-76ba-4585-a67b-bad177054a6c",
   "metadata": {},
   "source": [
    "The above are the tokens found in the given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0758e-030c-4e31-99d3-bfdf97bbcb2e",
   "metadata": {},
   "source": [
    "## Stemming and Lowering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab590efe-3cfe-4995-8815-6bf9ceeeef83",
   "metadata": {},
   "source": [
    "**Stemming** is the process of reducing words to their root form by removing prefixes or suffixes, without considering the actual meaning.  \n",
    "\n",
    "For example:  \n",
    "- **\"running\" → \"run\"**  \n",
    "- **\"flies\" → \"fli\"** (incorrect but common in stemming)  \n",
    "- **\"better\" → \"better\"** (unchanged since it doesn’t use linguistic rules)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aee8a66c-bf4d-41fd-a228-0318a81d6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a683fc7-80d2-42df-ac5b-f47370e15a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(word):\n",
    "    return stemmer.stem(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "af18cdc4-e09c-4aca-89cd-561952899f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Organized', 'Organizer', 'Organizing']\n"
     ]
    }
   ],
   "source": [
    "words = ['Organized', 'Organizer', 'Organizing']\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7c5638c-c312-4d19-9d12-be9d5cd8015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['organ', 'organ', 'organ']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [stemming(w) for w in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fd4fe-758c-4d28-bd25-49f6a75ab151",
   "metadata": {},
   "source": [
    "The above output words are the stemmed versions of the listed words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c7498-69e2-4449-b7d0-9195ebc00c80",
   "metadata": {},
   "source": [
    "## Creating train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28118d96-d467-427a-ba21-6802b1a4a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Datasets/intents.json', 'r') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "61b1c32f-db0b-4128-9b48-2c525305a8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi',\n",
       "    'Hey',\n",
       "    'How are you',\n",
       "    'Is anyone there?',\n",
       "    'Hello',\n",
       "    'Good day'],\n",
       "   'responses': ['Hey :-)',\n",
       "    'Hello, thanks for visiting',\n",
       "    'Hi there, what can I do for you?',\n",
       "    'Hi there, how can I help?']},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
       "   'responses': ['See you later, thanks for visiting',\n",
       "    'Have a nice day',\n",
       "    'Bye! Come back again soon.']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['Thanks', 'Thank you', \"That's helpful\", \"Thank's a lot!\"],\n",
       "   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n",
       "  {'tag': 'items',\n",
       "   'patterns': ['Which items do you have?',\n",
       "    'What kinds of items are there?',\n",
       "    'What do you sell?'],\n",
       "   'responses': ['We sell coffee and tea', 'We have coffee and tea']},\n",
       "  {'tag': 'payments',\n",
       "   'patterns': ['Do you take credit cards?',\n",
       "    'Do you accept Mastercard?',\n",
       "    'Can I pay with Paypal?',\n",
       "    'Are you cash only?'],\n",
       "   'responses': ['We accept VISA, Mastercard and Paypal',\n",
       "    'We accept most major credit cards, and Paypal']},\n",
       "  {'tag': 'delivery',\n",
       "   'patterns': ['How long does delivery take?',\n",
       "    'How long does shipping take?',\n",
       "    'When do I get my delivery?'],\n",
       "   'responses': ['Delivery takes 2-4 days', 'Shipping takes 2-4 days']},\n",
       "  {'tag': 'funny',\n",
       "   'patterns': ['Tell me a joke!',\n",
       "    'Tell me something funny!',\n",
       "    'Do you know a joke?'],\n",
       "   'responses': ['Why did the hipster burn his mouth? He drank the coffee before it was cool.',\n",
       "    'What did the buffalo say when his son left for college? Bison.']}]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa484fbd-d941-4b04-b624-0cca817f797d",
   "metadata": {},
   "source": [
    "For applying bag of words we need to first collect all the words present in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5c1af951-bb12-40a0-8358-82b426a73d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []    #List of all the tokens\n",
    "tags = []         #List of all the tags in the training data\n",
    "xy = []           #Combination of all the words and it's respective tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21322f-0990-4544-b767-5501956f7c7e",
   "metadata": {},
   "source": [
    "### Tokenizing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44ca55cb-71e7-43a2-b6d5-f3db1d0f265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in train_data['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)    #We use the `extend` function because we want to avoid a list of lists and instead gather \n",
    "                               #all the tokens from the pattern into a single list.\n",
    "        xy.append((w, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "28013260-9ed5-4dbe-ac11-d16cae04121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Hey', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'Thank', \"'s\", 'a', 'lot', '!', 'Which', 'items', 'do', 'you', 'have', '?', 'What', 'kinds', 'of', 'items', 'are', 'there', '?', 'What', 'do', 'you', 'sell', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Can', 'I', 'pay', 'with', 'Paypal', '?', 'Are', 'you', 'cash', 'only', '?', 'How', 'long', 'does', 'delivery', 'take', '?', 'How', 'long', 'does', 'shipping', 'take', '?', 'When', 'do', 'I', 'get', 'my', 'delivery', '?', 'Tell', 'me', 'a', 'joke', '!', 'Tell', 'me', 'something', 'funny', '!', 'Do', 'you', 'know', 'a', 'joke', '?']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "963b8aa0-1875-41a8-bc22-a9f7b9d08703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1406bd-8669-4a82-becb-1f62f7fbe70c",
   "metadata": {},
   "source": [
    "There are 103 tokens in total in our training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08dab12-ddde-4243-9626-2fe20f53c1e0",
   "metadata": {},
   "source": [
    "### Adding Punctuations to ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "528cdf7e-4f51-4c93-b921-995530095a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words = ['@', '#', '$', '%', '&', '*', '(', ')', '?', '!', '.', ',']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d00a9-f412-4f4a-a59c-e978073b6eca",
   "metadata": {},
   "source": [
    "### Stemming and removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df75dfe7-bb36-4f4c-abe4-ae7be792f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [stemming(word) for word in all_words if word not in ignore_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "982db9e7-f40c-4a9d-8136-f16ba5476d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hey', 'how', 'are', 'you', 'is', 'anyon', 'there', 'hello', 'good', 'day', 'bye', 'see', 'you', 'later', 'goodby', 'thank', 'thank', 'you', 'that', \"'s\", 'help', 'thank', \"'s\", 'a', 'lot', 'which', 'item', 'do', 'you', 'have', 'what', 'kind', 'of', 'item', 'are', 'there', 'what', 'do', 'you', 'sell', 'do', 'you', 'take', 'credit', 'card', 'do', 'you', 'accept', 'mastercard', 'can', 'i', 'pay', 'with', 'paypal', 'are', 'you', 'cash', 'onli', 'how', 'long', 'doe', 'deliveri', 'take', 'how', 'long', 'doe', 'ship', 'take', 'when', 'do', 'i', 'get', 'my', 'deliveri', 'tell', 'me', 'a', 'joke', 'tell', 'me', 'someth', 'funni', 'do', 'you', 'know', 'a', 'joke']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "162deeb1-4254-4d69-9151-c04e5a338138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488771-99fb-478f-8793-fe1377008416",
   "metadata": {},
   "source": [
    "The length of the list remains the same, but the words are now changed to their respective root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "903522f8-d74a-4100-98ba-1b647a7dc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa6811-a8e3-4955-bc5b-2a7ea542e30f",
   "metadata": {},
   "source": [
    "Stemming can produce the same root word for multiple words, leading to duplicates in the token list. To handle this, we convert the tokens into a set to remove duplicates and then use the `sorted` function to return a sorted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3c22a330-47ca-4742-9f2b-ddc5870eb775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'a', 'accept', 'anyon', 'are', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'deliveri', 'do', 'doe', 'funni', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'i', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'my', 'of', 'onli', 'pay', 'paypal', 'see', 'sell', 'ship', 'someth', 'take', 'tell', 'thank', 'that', 'there', 'what', 'when', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2d58733-a5ec-4115-9bce-f2a7cdc35949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33defb2f-53cc-423d-887a-f2bc189de09d",
   "metadata": {},
   "source": [
    "The total length reduces after removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b65f181c-5b04-429d-93bb-434da519e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = sorted(set(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05f5ab35-8003-435c-acf7-7615763587e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "95b675a9-e056-478f-a1b8-ebaa15da1451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Hi'], 'greeting'),\n",
       " (['Hey'], 'greeting'),\n",
       " (['How', 'are', 'you'], 'greeting'),\n",
       " (['Is', 'anyone', 'there', '?'], 'greeting'),\n",
       " (['Hello'], 'greeting'),\n",
       " (['Good', 'day'], 'greeting'),\n",
       " (['Bye'], 'goodbye'),\n",
       " (['See', 'you', 'later'], 'goodbye'),\n",
       " (['Goodbye'], 'goodbye'),\n",
       " (['Thanks'], 'thanks'),\n",
       " (['Thank', 'you'], 'thanks'),\n",
       " (['That', \"'s\", 'helpful'], 'thanks'),\n",
       " (['Thank', \"'s\", 'a', 'lot', '!'], 'thanks'),\n",
       " (['Which', 'items', 'do', 'you', 'have', '?'], 'items'),\n",
       " (['What', 'kinds', 'of', 'items', 'are', 'there', '?'], 'items'),\n",
       " (['What', 'do', 'you', 'sell', '?'], 'items'),\n",
       " (['Do', 'you', 'take', 'credit', 'cards', '?'], 'payments'),\n",
       " (['Do', 'you', 'accept', 'Mastercard', '?'], 'payments'),\n",
       " (['Can', 'I', 'pay', 'with', 'Paypal', '?'], 'payments'),\n",
       " (['Are', 'you', 'cash', 'only', '?'], 'payments'),\n",
       " (['How', 'long', 'does', 'delivery', 'take', '?'], 'delivery'),\n",
       " (['How', 'long', 'does', 'shipping', 'take', '?'], 'delivery'),\n",
       " (['When', 'do', 'I', 'get', 'my', 'delivery', '?'], 'delivery'),\n",
       " (['Tell', 'me', 'a', 'joke', '!'], 'funny'),\n",
       " (['Tell', 'me', 'something', 'funny', '!'], 'funny'),\n",
       " (['Do', 'you', 'know', 'a', 'joke', '?'], 'funny')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc9c64-fa95-4541-bc73-227b37d16524",
   "metadata": {},
   "source": [
    "### Creating Bag of Words(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b24f2ce-d85f-4518-87e5-f94f69a93a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "679e54cb-d0af-4340-94a0-6847b86836f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(pattern_sequences, all_words):\n",
    "    tokenized_sentence = [stemming(w) for w in pattern_sequences]  #Applying Stemming for all the words\n",
    "    bow = np.zeros(len(all_words), dtype=np.float32)\n",
    "    for idx, w in enumerate(all_words):\n",
    "        if w in tokenized_sentence:\n",
    "            bow[idx]+=1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "407c2848-6db5-4ab2-888a-7191b975a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b40109-301b-4ccd-a693-fea27c6df9e9",
   "metadata": {},
   "source": [
    "Here, `y_train` doesn't need to be in one-hot encoded form because we are using **CrossEntropyLoss**, which expects class index labels instead of one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0c99ab79-e80b-45d7-b7a1-782472e57435",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "804a69b9-770e-4b23-940b-eeccb254d9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf83681b-7fb4-43a8-a10f-ac7576f3b0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 2, 2, 2, 6, 6, 6, 6, 4, 4, 4, 5, 5, 5, 5, 0, 0,\n",
       "       0, 1, 1, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f40f70-9278-418d-8cd0-1c5934d20a83",
   "metadata": {},
   "source": [
    "### Creating a PyTorch Dataset using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ca4b020-09b3-40d4-9a2c-b62aaf8a9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.X_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf633a-fd3b-4f93-89b9-2d1371d8b61a",
   "metadata": {},
   "source": [
    "- Inherits from `Dataset`, a PyTorch class for handling custom datasets.  \n",
    "- The `__init__` method initializes the dataset, storing `X_train` (features) and `y_train` (labels).  \n",
    "- `self.n_samples = len(X_train)` stores the total number of samples in the dataset.  \n",
    "- The `__getitem__` method takes an `index` and returns the corresponding feature (`X_train[index]`) and label (`y_train[index]`).  \n",
    "- The `__len__` method returns `self.n_samples`, which helps PyTorch determine the dataset size.  \n",
    "- This class is useful when used with `DataLoader` for batch processing and efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bc2e4-b8c4-4824-aa01-3d7dfc2261ee",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "130e8038-cdea-45be-b00b-f9ce764f1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "num_classes = len(tags)\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5afad-cf48-4c6e-b4cf-f72e7e4badba",
   "metadata": {},
   "source": [
    "### Creation of Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f298bb2-16d4-4490-aaad-982baad7dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataSet()\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070212e-4cd9-473f-a4d3-df67939990a8",
   "metadata": {},
   "source": [
    "- `batch_size = 8` sets the number of samples per batch during training.  \n",
    "- `dataset = ChatDataSet()` creates an instance of the `ChatDataSet` class, which holds the training data.  \n",
    "- `DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)` creates a `DataLoader` to handle batching, shuffling, and sampling of data.  \n",
    "- `dataset=dataset` tells the `DataLoader` which dataset to use.  \n",
    "- `batch_size=batch_size` ensures that each batch contains 8 samples.  \n",
    "- `shuffle=True` randomizes the order of samples in each epoch to improve generalization.  \n",
    "- This setup is useful for efficient training and feeding data in mini-batches to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925aae4-0a6d-42d1-b188-f961d52e0cbc",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "faabe300-97bb-4ffe-bd0a-d4a5301c6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5045d653-bfe7-4262-942c-6eaefe28d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043dd4d0-426c-4989-9016-8753cd2f5295",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "087b00c3-3c20-43d4-9294-5e84315aa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b63254d0-563d-4c6e-92e8-75365c0daf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4920f7-1a11-4a3b-96eb-8fdafa1201e8",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9743c8d7-3138-469e-8088-b5d2914ec370",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "552f4a31-bafd-4b07-bf85-7fe251281a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, loss=1.0290\n",
      "Epoch 200/1000, loss=0.5270\n",
      "Epoch 300/1000, loss=0.0329\n",
      "Epoch 400/1000, loss=0.0215\n",
      "Epoch 500/1000, loss=0.4692\n",
      "Epoch 600/1000, loss=0.0027\n",
      "Epoch 700/1000, loss=0.0056\n",
      "Epoch 800/1000, loss=0.2505\n",
      "Epoch 900/1000, loss=0.0017\n",
      "Epoch 1000/1000, loss=0.0020\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "\n",
    "        # Forward Pass\n",
    "        output = model(words)\n",
    "        loss = criterion(output, labels)\n",
    "    \n",
    "        # Backward Pass and Optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, loss={loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753abe57-1f8a-4459-8e0a-0bd7e615c8ad",
   "metadata": {},
   "source": [
    "## Saving the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "19bd21de-9286-4379-a223-e618a6367b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model_state\":model.state_dict(),\n",
    "    \"input_size\":input_size,\n",
    "    \"output_size\":num_classes,\n",
    "    \"hidden_size\":hidden_size,\n",
    "    \"all_words\":all_words,\n",
    "    \"tags\":tags\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "48ec8e66-2b80-4b6c-92b9-9aa7df041926",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b2c8bed0-cc96-4f85-aa6c-71bfec4f624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afb25f-f003-4a75-baee-e6719b580937",
   "metadata": {},
   "source": [
    "## Implementation of Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "53d62219-58bc-4074-8e44-94781caa9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Datasets/intents.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ceb41815-6b05-4d3b-8ce1-f6561b784708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvyas\\AppData\\Local\\Temp\\ipykernel_17572\\2703154485.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file)\n"
     ]
    }
   ],
   "source": [
    "data = torch.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6ab31e87-2ce4-409c-8d32-803570f2b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's chat! (type 'exit' to quit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hello, this is vedavyas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hi there, how can I help?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I want to greet you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I do not understand...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Thank you for working\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: My pleasure\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Bye! Come back again soon.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "input_size = data[\"input_size\"]\n",
    "hidden_size = data[\"hidden_size\"]\n",
    "output_size = data[\"output_size\"]\n",
    "all_words = data['all_words']\n",
    "tags = data['tags']\n",
    "model_state = data[\"model_state\"]\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "model.load_state_dict(model_state)\n",
    "model.eval()\n",
    "\n",
    "bot_name = \"Bot\"\n",
    "print(\"Let's chat! (type 'exit' to quit)\")\n",
    "while True:\n",
    "    # sentence = \"do you use credit cards?\"\n",
    "    sentence = input(\"You: \")\n",
    "    if sentence == \"exit\":\n",
    "        break\n",
    "\n",
    "    sentence = tokenize(sentence)\n",
    "    X = bag_of_words(sentence, all_words)\n",
    "    X = X.reshape(1, X.shape[0])\n",
    "    X = torch.from_numpy(X).to(device)\n",
    "\n",
    "    output = model(X)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    tag = tags[predicted.item()]\n",
    "\n",
    "    probs = torch.softmax(output, dim=1)\n",
    "    prob = probs[0][predicted.item()]\n",
    "    if prob.item() > 0.75:\n",
    "        for intent in intents['intents']:\n",
    "            if tag == intent[\"tag\"]:\n",
    "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
    "    else:\n",
    "        print(f\"{bot_name}: I do not understand...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
